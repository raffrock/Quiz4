{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvcoyqCM73pj"
      },
      "source": [
        "* Created GitHub repository with descriptive name\n",
        "* Uploaded code (.ipynb or .py file)\n",
        "* Created comprehensive README.md with:\n",
        "* Project description\n",
        "* Architecture explanation\n",
        "How to run the code\n",
        "* cleanedctexts and analysis\n",
        "* Team member contributions\n",
        "* Added requirements.txt or environment.yml (if needed)\n",
        "* Tested that repository is public and accessible\n",
        "* Prepared 2-minute presentation\n",
        "* All team members understand the code\n",
        "* Submitted GitHub link to Brightspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XPdDXY88Gqh"
      },
      "source": [
        "Goal\n",
        "Build a character-level or word-level text generator that learns to write in a particular style\n",
        "using RNN/LSTM.\n",
        "Minimal Viable Implementation\n",
        "1. Choose a text corpus (Shakespeare, song lyrics, etc.)\n",
        "2. Build character-level or word-level RNN/LSTM\n",
        "3. Train the model to predict next character/word\n",
        "4. Generate new text by sampling from the model\n",
        "Suggested Datasets\n",
        "• Shakespeare text (small, classic choice)\n",
        "• Your favorite song lyrics (personal touch!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pip 21.2.4 from /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages/pip (python 3.9)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.20.0-cp39-cp39-macosx_12_0_arm64.whl (200.4 MB)\n",
            "Collecting grpcio<2.0,>=1.24.3\n",
            "  Downloading grpcio-1.76.0-cp39-cp39-macosx_11_0_universal2.whl (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 6.2 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
            "Collecting keras>=3.10.0\n",
            "  Downloading keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 4.9 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting termcolor>=1.1.0\n",
            "  Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
            "Collecting requests<3,>=2.21.0\n",
            "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 6.0 MB/s  eta 0:00:01\n",
            "\u001b[?25hCollecting wrapt>=1.11.0\n",
            "  Downloading wrapt-2.0.1-cp39-cp39-macosx_11_0_arm64.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 540 kB/s  eta 0:00:01\n",
            "\u001b[?25hCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
            "  Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Collecting google_pasta>=0.1.1\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 19.6 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting opt_einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 1.5 MB/s  eta 0:00:01\n",
            "\u001b[?25hCollecting flatbuffers>=24.3.25\n",
            "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
            "Collecting tensorboard~=2.20.0\n",
            "  Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 1.5 MB/s eta 0:00:011\n",
            "\u001b[?25hCollecting absl-py>=1.0.0\n",
            "  Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
            "Requirement already satisfied: packaging in /Users/rchetata/Library/Python/3.9/lib/python/site-packages (from tensorflow) (25.0)\n",
            "Collecting astunparse>=1.6.0\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting ml_dtypes<1.0.0,>=0.5.1\n",
            "  Downloading ml_dtypes-0.5.4-cp39-cp39-macosx_10_9_universal2.whl (676 kB)\n",
            "\u001b[K     |████████████████████████████████| 676 kB 2.4 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
            "Collecting libclang>=13.0.0\n",
            "  Using cached libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
            "Collecting protobuf>=5.28.0\n",
            "  Downloading protobuf-6.33.1-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
            "\u001b[K     |████████████████████████████████| 427 kB 3.2 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=3.6.6 in /Users/rchetata/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /Users/rchetata/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.0.2)\n",
            "Collecting h5py>=3.11.0\n",
            "  Downloading h5py-3.14.0-cp39-cp39-macosx_11_0_arm64.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 5.1 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
            "Collecting namex\n",
            "  Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
            "Collecting optree\n",
            "  Downloading optree-0.18.0-cp39-cp39-macosx_11_0_arm64.whl (330 kB)\n",
            "\u001b[K     |████████████████████████████████| 330 kB 3.9 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting rich\n",
            "  Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 7.3 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting charset_normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.4.4-cp39-cp39-macosx_10_9_universal2.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 4.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 5.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 5.2 MB/s eta 0:00:011\n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
            "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "\u001b[K     |████████████████████████████████| 129 kB 3.8 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading markdown-3.9-py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 5.9 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
            "Collecting werkzeug>=1.0.1\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[K     |████████████████████████████████| 224 kB 8.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting pillow\n",
            "  Downloading pillow-11.3.0-cp39-cp39-macosx_11_0_arm64.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 2.5 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.4 in /Users/rchetata/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard~=2.20.0->tensorflow) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /Users/rchetata/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.20.0->tensorflow) (3.23.0)\n",
            "Collecting MarkupSafe>=2.1.1\n",
            "  Downloading markupsafe-3.0.3-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
            "Collecting markdown-it-py>=2.2.0\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 3.3 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/rchetata/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: mdurl, MarkupSafe, markdown-it-py, werkzeug, urllib3, tensorboard-data-server, rich, protobuf, pillow, optree, namex, ml-dtypes, markdown, idna, h5py, grpcio, charset-normalizer, certifi, absl-py, wrapt, termcolor, tensorboard, requests, opt-einsum, libclang, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
            "Successfully installed MarkupSafe-3.0.3 absl-py-2.3.1 astunparse-1.6.3 certifi-2025.11.12 charset-normalizer-3.4.4 flatbuffers-25.9.23 gast-0.6.0 google-pasta-0.2.0 grpcio-1.76.0 h5py-3.14.0 idna-3.11 keras-3.10.0 libclang-18.1.1 markdown-3.9 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.4 namex-0.1.0 opt-einsum-3.4.0 optree-0.18.0 pillow-11.3.0 protobuf-6.33.1 requests-2.32.5 rich-14.2.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.1.0 urllib3-2.5.0 werkzeug-3.1.3 wrapt-2.0.1\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: numpy in /Users/rchetata/Library/Python/3.9/lib/python/site-packages (2.0.2)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# !pip3 -V\n",
        "# !pip3 install tensorflow\n",
        "# !pip3 install numpy\n",
        "# !pip3 install re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYO3pJAeyzGf",
        "outputId": "61997579-fb15-4404-eab7-b68acb58a656"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/rchetata/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "[' ', '3', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "import re\n",
        "# import time\n",
        "\n",
        "# 1. Prepare text data\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='UTF-8')\n",
        "\n",
        "text_list = text.split('\\n')\n",
        "\n",
        "text = text.lower()\n",
        "texts = re.sub(r'[^\\w\\s]','', text) # removes nonwords or spaces\n",
        "cleaned_text = re.sub(r'[\\n]',' ', texts)\n",
        "vocab = sorted(set(cleaned_text))\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-a1Ocg7rwNwr",
        "outputId": "67e36f60-cae0-4f91-9497-3056de3ab682"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([b'f' b'i' b'r' ... b'n' b'g' b' '], shape=(1060997,), dtype=string)\n",
            "28\n",
            "29\n",
            "tf.Tensor([ 8 11 20 ... 16  9  1], shape=(1060997,), dtype=int64)\n",
            "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
            "<_BatchDataset element_spec=TensorSpec(shape=(30,), dtype=tf.int64, name=None)>\n"
          ]
        }
      ],
      "source": [
        "# Tokenize and create sequences\n",
        "sequence_length = 29 # to fit vocab length\n",
        "\n",
        "chars = tf.strings.unicode_split(cleaned_text, input_encoding='UTF-8', errors='ignore')\n",
        "print(chars)\n",
        "\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n",
        "ids = ids_from_chars(chars)\n",
        "print(len(list(vocab)))\n",
        "print(len(ids_from_chars.get_vocabulary()))\n",
        "\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), encoding='UTF-8', invert=True, mask_token=None)\n",
        "# print(chars_from_ids)\n",
        "\n",
        "all_ids = ids_from_chars(chars)\n",
        "print(all_ids)\n",
        "\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "print(ids_dataset)\n",
        "\n",
        "sequences = ids_dataset.batch(sequence_length+1, drop_remainder=True)\n",
        "print(sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZLZkhqu8lWQ",
        "outputId": "d74e94c2-3d62-4a05-cdd9-fa188c5164fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'first citizen before we procee'\n",
            "b'd any further hear me speak  a'\n",
            "b'll speak speak  first citizen '\n",
            "b'you are all resolved rather to'\n",
            "b' die than to famish  all resol'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-19 21:49:32.226498: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        }
      ],
      "source": [
        "# checking to see if sequences works\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "\n",
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwpY1uHvxDG6",
        "outputId": "4a9e2e08-f4fb-426e-fcd0-7a5e67b16dd2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/rchetata/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# 2. Build LSTM model\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 256\n",
        "lstm_units = 512\n",
        "model = Sequential([\n",
        "Embedding(vocab_size, embedding_dim,\n",
        "input_length=sequence_length),\n",
        "# two LSTM layers\n",
        "LSTM(lstm_units, return_sequences=True),\n",
        "LSTM(lstm_units),\n",
        "Dense(vocab_size, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzMTbwVeyqt0",
        "outputId": "6965a3a6-a36b-43af-c30b-494ffaf77237"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<_MapDataset element_spec=(TensorSpec(shape=(29,), dtype=tf.int64, name=None), TensorSpec(shape=(29,), dtype=tf.int64, name=None))>\n",
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 29), dtype=tf.int64, name=None), TensorSpec(shape=(64, 29), dtype=tf.int64, name=None))>\n",
            "Epoch 1/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 734ms/step - loss: 1448.5936\n",
            "Epoch 2/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 734ms/step - loss: 1448.5936\n",
            "Epoch 2/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 697ms/step - loss: 1584.2043\n",
            "Epoch 3/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 697ms/step - loss: 1584.2043\n",
            "Epoch 3/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 683ms/step - loss: 1580.0298\n",
            "Epoch 4/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 683ms/step - loss: 1580.0298\n",
            "Epoch 4/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m477s\u001b[0m 864ms/step - loss: 1559.8605\n",
            "Epoch 5/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m477s\u001b[0m 864ms/step - loss: 1559.8605\n",
            "Epoch 5/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 695ms/step - loss: 1562.5664\n",
            "Epoch 6/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 695ms/step - loss: 1562.5664\n",
            "Epoch 6/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 654ms/step - loss: 1592.0016\n",
            "Epoch 7/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 654ms/step - loss: 1592.0016\n",
            "Epoch 7/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 677ms/step - loss: 1552.1947\n",
            "Epoch 8/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 677ms/step - loss: 1552.1947\n",
            "Epoch 8/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 670ms/step - loss: 1551.5686\n",
            "Epoch 9/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 670ms/step - loss: 1551.5686\n",
            "Epoch 9/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 509ms/step - loss: 1552.8704\n",
            "Epoch 10/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 509ms/step - loss: 1552.8704\n",
            "Epoch 10/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 501ms/step - loss: 1551.2084\n",
            "Epoch 11/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 501ms/step - loss: 1551.2084\n",
            "Epoch 11/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 502ms/step - loss: 1558.9037\n",
            "Epoch 12/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 502ms/step - loss: 1558.9037\n",
            "Epoch 12/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 521ms/step - loss: 1553.4713\n",
            "Epoch 13/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 521ms/step - loss: 1553.4713\n",
            "Epoch 13/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 607ms/step - loss: 1502.2169\n",
            "Epoch 14/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 607ms/step - loss: 1502.2169\n",
            "Epoch 14/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 613ms/step - loss: 1547.3906\n",
            "Epoch 15/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 613ms/step - loss: 1547.3906\n",
            "Epoch 15/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2015s\u001b[0m 4s/step - loss: 1555.3802\n",
            "Epoch 16/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2015s\u001b[0m 4s/step - loss: 1555.3802\n",
            "Epoch 16/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 702ms/step - loss: 1486.8448\n",
            "Epoch 17/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 702ms/step - loss: 1486.8448\n",
            "Epoch 17/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 543ms/step - loss: 1553.9751\n",
            "Epoch 18/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 543ms/step - loss: 1553.9751\n",
            "Epoch 18/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 555ms/step - loss: 1538.5118\n",
            "Epoch 19/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 555ms/step - loss: 1538.5118\n",
            "Epoch 19/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 590ms/step - loss: 1538.3284\n",
            "Epoch 20/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 590ms/step - loss: 1538.3284\n",
            "Epoch 20/20\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 542ms/step - loss: 1546.8319\n",
            "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 542ms/step - loss: 1546.8319\n",
            "<keras.src.callbacks.history.History object at 0x162b31d90>\n",
            "<keras.src.callbacks.history.History object at 0x162b31d90>\n"
          ]
        }
      ],
      "source": [
        "# 3. Train model\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return (input_text, target_text)\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "print(dataset)\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "dataset_batched = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "print(dataset_batched)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "history = model.fit(dataset_batched, batch_size=BATCH_SIZE, epochs=20)\n",
        "print(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the mind is its own place and in itself can make a heaven of hell a hell of heaven hm cc   c c chhc  ht c n yhdcch cchc hcl chmchh ncchco nchchhhc chrc chc ko  ca zy amhc ccha h ck\n",
            "abashed the devil stood and felt how awful goodness is and saw virtue in her shape how lovely and pined his losshc hc  cm  3y mrh   dn canyhh hh hccchmkc  h c ac ch fz chhhc achh  ncc     hlmv l c3kc 3hkwhhhc \n",
            "abashed the devil stood and felt how awful goodness is and saw virtue in her shape how lovely and pined his losshc hc  cm  3y mrh   dn canyhh hh hccchmkc  h c ac ch fz chhhc achh  ncc     hlmv l c3kc 3hkwhhhc \n",
            "all is not lost the unconquerable will and study of revenge immortal hate and the courage never to submit or yield h ntc a hhc  lr zkjk hczcnchncn ch chhacc jh ah hch hhnrc     hhhc  h hyac h    a n czhac    h h\n",
            "all is not lost the unconquerable will and study of revenge immortal hate and the courage never to submit or yield h ntc a hhc  lr zkjk hczcnchncn ch chhacc jh ah hch hhnrc     hhhc  h hyac h    a n czhac    h h\n"
          ]
        }
      ],
      "source": [
        "def generate_text(seed, length=200, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generates text character-by-character using the trained LSTM model.\n",
        "    'seed' is the starting text.\n",
        "    'length' is how many new characters to generate.\n",
        "    'temperature' controls randomness (lower = less random, higher = more random).\n",
        "    \"\"\"\n",
        "\n",
        "    # clean seed\n",
        "    seed = seed.lower()\n",
        "    text = re.sub(r'[^\\w\\s]','', seed) # removes nonwords or spaces\n",
        "    cleaned_text = re.sub(r'[\\n]',' ', text)\n",
        "\n",
        "    # Convert each seed character to its integer ID.\n",
        "    char_to_id = ids_from_chars(tf.strings.unicode_split(cleaned_text, 'UTF-8')).numpy().tolist()\n",
        "\n",
        "    for l in range(length):\n",
        "        sample_window = char_to_id[-sequence_length:]\n",
        "        if len(sample_window) < sequence_length:\n",
        "            pad_len = sequence_length - len(sample_window)\n",
        "            sample_window = [0] * pad_len + sample_window\n",
        "\n",
        "        sample_window = np.array(sample_window).reshape(1, -1)\n",
        "\n",
        "        prediction = model.predict(sample_window, verbose=0)[0]\n",
        "\n",
        "        prediction = np.log(prediction + 1e-8) / temperature\n",
        "        exp_preds = np.exp(prediction)\n",
        "        prediction = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "        # Randomly choose a character index according to the probability distribution.\n",
        "        next_idx = np.random.choice(range(vocab_size), p=prediction)\n",
        "        next_char = chars_from_ids(tf.constant([next_idx])).numpy()[0].decode('utf-8')\n",
        "\n",
        "        # Append the new character to both the numeric sequence and the output string.\n",
        "        if next_char != \"[UNK]\":\n",
        "            char_to_id.append(next_idx)\n",
        "            cleaned_text += next_char\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# test with paradise lost\n",
        "seed1 = \"The mind is its own place, and in itself can make a heaven of hell, a hell of heaven.\"\n",
        "seed2 = \"Abashed the devil stood and felt how awful goodness is and saw Virtue in her shape how lovely: and pined his loss\"\n",
        "seed3 =  \"All is not lost, the unconquerable will, and study of revenge, immortal hate, and the courage never to submit or yield\"\n",
        "print(generate_text(seed1, length=100, temperature=0.8))\n",
        "print(generate_text(seed2, length=100, temperature=0.8))\n",
        "print(generate_text(seed3, length=100, temperature=0.8))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
